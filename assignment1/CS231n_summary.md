# **对于 Assignment1 各个模型的综述**

## *1. KNN*

### **实现细节及数学原理:**

1. 计算训练样本和测试样本'距离'(**各特征值视为坐标值，相减平方开根号**)然后找出距离最小的前k个的所属类，在找出出现最多的类，即预测结果
2. **k折交叉验证**: 对于每一个K，取正整数n，把训练样本分为n组，并遍历，把当前组作为新测试样本，剩下n-1组作为新训练样本，重复当前k的预测步骤，并把这n次的正确率放入列表以便比较，可得到最佳k取值
3. 涉及方法: 利用广播**完全向量化**计算距离; **argsort**; **argmax**; **bincount**; **array_spilt**; **concatenate**

### **性能比较分析:**

不需要训练，简单直接，但训练正确率最低

### **困难及解决方案:**

1. 用完全向量化求距离时，用newaxis方法，形成三维广播但本地运行内存不允许，最终改用拆解完全平方式
2. 运用的方法大多先前没用过，理解方法耗费一定时间

## *2. Softmax*

### **实现细节及数学原理:**

1. 运用线性变换scores = wx + b,得到分数运用softmax通过指数化,归一化把分数转换成概率，用交叉熵函数求损失,对概率加以处理得到误差矩阵，最后加以正则化得到最终损失和梯度
2. **随机梯度下降**: 对于每一次w迭代都不直接取所有训练样本而是从num_train中随机可重复抽取获得列表型索引,取出特定数量的样本,并对这些样本进行先前的线性变换等操作
3. 涉及方法: **点乘**, **random.choice**, **列表索引法**

### **性能比较分析:**

运用了线性变换,需要迭代训练了，较于knn有性能上的较小提升,但只有线性的性能必然是局限的

### **困难及解决方案:**

1. 运用点乘，数组维度匹配问题频出，不过学到了聚合函数里用的keepdims=True，维持维度
2. 对于交叉熵函数和误差矩阵最后一步的计算，与先前理解的方法有所出入，比如损失只取对应y_train预测类的概率，误差矩阵只让对应y_train预测类概率减1
3. 对于正则化算数式及目的的理解耗费一定时间,询问ai得到惩罚机制的相关解释

## *3. Two_layer_net*

### **实现细节及数学原理 (隐藏层与反向传播):**

1. 先前直接用softmax激活，两层神经网络就是在此前加上**两个隐藏层**对样本特征进行处理此为**正向传播**过程，并且还多了一个**反向传播**过程，反向传播过程中不断返回dx，求得参数梯度，回到原点进行参数更新优化，两层也意味着有2套权重偏置参数
2. 重要的**分步思想**: 网络实现大体分为 **forward** 和 **backward** 函数。正向传播顺序是 **全连接层1+ReLu激活 -> 全连接层2 -> 输出层(softmax激活)**,输出层得到误差矩阵和损失，然后按反方向计算前一层的dx,和本层的dw1,dw2,db1,db2(**记得正则化!!!!**)此过程封装为model的**loss函数**，返回loss和grad列表
3. **求解器solver**: 将**train**,**check_accuracy**,**step(实现SGD,并更新参数)** 合并在一起,对模型进行训练,需要注意的是算准确率时也需要分批，**在抽取的的样本里再根据batch_size分若干批进行预测** ，再将预测向量拼起来，以及迭代时num_iters的计算，取决于**epoch(学习率衰减和计算正确率的周期)**
4. **训练参数策略**: 更大的训练集有利于模型学习的广泛性，一般性，模型刷更多题自然会变聪明。
增加隐藏单元，容易导致模型容量过大，相当于模型平时刷的题太复杂，平时训练做的特别，考试时会太过自信，导致训练和测试时正确率差距更大。
增加正则化强度确实会缩小差距，严师出高徒，一般在正确率都不高时，不采用正则化效果好点

### **性能比较分析:**

两层神经网络相比之前已经复杂了极多，具有非线性激活，效能自然也上升显著

### **困难及解决方案:**

1. 刚开始对于两层神经网络的具体架构缺乏形象化逻辑，发现综合看一遍文件再问ai确认，就大概明白
2. 训练过程复杂了许多，流程添加了挺多，花费较多时间整理思路
3. 不同激活函数的优缺点还是有点晕晕绕绕

## *4. Features*

### **实现细节及数学原理 (HOG与HSV特征提取):**

1. 大致分为两种抽象特征提取，一个是边缘特征(**HOG**)，一个是色调特征(**HSV**),**HOG**就是将一个有若干像素的RGB图片变为灰白然后再分块成各个单元格，然后求出每个单元格在不同方向的**梯度幅值**均值，并储存在新矩阵，然后再展平成一维向量;**HSV**关键运用matplotlib功能将RGB通道转为色调，饱和度，明度三个通道，**(此处只取色调这个通道)** 然后运用histogram方法求出一个像素在不同色调区间的概率
2. 涉及方法: **diff**, **uniform_filter**, **ravel**, **arctan2**, **squeeze**, **floor**, **ndim**, **assert**

### **性能比较分析:**

配合softmax能达到两层神经网络的正确率，不过比较依赖特征提取的质量

### **困难及解决方案:**

1. 运用的方法很多而且思路从未了解过，看到时一脸茫然，这个依赖AI理解的时间较多，特别是对梯度幅值的理解和unifom_filter的使用
2. 不懂为啥arctan2的结果还要加个90度

## *5. Fully_connected_nets*

### **实现细节及数学原理:**

1. 其实是**多层神经网络**，增加了隐藏单元个数，因此**hidden_dims**是个列表 **(要在开头结尾拼接加上输入维度和类别数,这样每次取两个就得到每个W的初始化形状)**，比较于两层，要求层数，要用**层数循环**作参数初始化，正向传播，反向传播，同时多了**dropout**和**batchnorm**两个新的正则方法，增加了除SGD外的其他**参数更新法则**
2. **多种参数更新法则**: **SGD**, **SGD+momentum**, **RMSprop**, **Adam**


### **性能比较分析:**

拥有更多的隐藏层,深度更深自然准确率凌驾于其他模型之上

### **困难及解决方案:**

1. 主要是多了**dropout**这层,mask的使用需要琢磨一会，并且前进后退都需要用到字典的mode参数，**默认**这个mode参数存在，但是如果一开始就没有用这个方法，mode就不存在，就会导致这层非常混乱，所以运行时要**再判断一次是否有用这个方法**，这个困住我好久，梯度检查一直失败
2. **多重cache**的保存有点眼花缭乱,dropout层和正常层的cache最好以元组为单位合并保存，每次取可以直接索引，不合并如果有none就会出现层与层取的cache不对等
3. 正则化我以为要根据层数除以不同的数，没想到都是1/2倍平方和
